{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd60452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085dabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256 \n",
    "BATCH_SIZE = 32\n",
    "CHANNEL = 3 \n",
    "EPOCHS = 10       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fe7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PlantVillage\",\n",
    "    shuffle=True,\n",
    "    image_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c39f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.class_names \n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1bd07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    print(image_batch[0].numpy())\n",
    "    print(image_batch.shape)\n",
    "    print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d46a1",
   "metadata": {},
   "source": [
    "# visualizaing image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8780e0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    plt.imshow(image_batch[0].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[label_batch[0]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cfc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "80% TRAINING \n",
    "20 % ==> 10% VALIDATION , 10% TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8 \n",
    "len(dataset)*train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.take(54)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481895c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = dataset.skip(54)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.1\n",
    "len(dataset)* val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab955fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = test_ds.take(6)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98540c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = test_ds.skip(6)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partition_tf(ds,train_split=0.8,val_split=0.1,test_split=0.1,\n",
    "                             shuffle=True,shuffle_size=1000):\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size,seed=12)\n",
    "    ds_size=len(ds)\n",
    "    train_size = int(train_split*ds_size)\n",
    "    val_size = int(val_split*ds_size)\n",
    "    train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds,val_ds,test_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7310e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,val_ds,test_ds = get_dataset_partition_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc8770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a94401",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2eaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38516d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fc8ab",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "Creating a Layer for Resizing and Normalization\n",
    "Before we feed our images to network, we should be resizing it to the desired size. Moreover, to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 256). This should happen while training as well as inference. Hence we can add that as a layer in our Sequential Model.\n",
    "\n",
    "You might be thinking why do we need to resize (256,256) image to again (256,256). You are right we don't need to but this will be useful when we are done with the training and start using the model for predictions. At that time somone can supply an image that is not (256,256) and this layer will resize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5f9d9",
   "metadata": {},
   "source": [
    "# Applying Data Augmentation to Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01010d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473a25c",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation.\n",
    "\n",
    "We are going to use convolutional neural network (CNN) here. CNN is popular for image classification tasks. Watch below video to understand fundamentals of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNEL)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32,(3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,(3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ef990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba624c",
   "metadata": {},
   "source": [
    "# Compiling the Model\n",
    "We use adam Optimizer, SparseCategoricalCrossentropy for losses, accuracy as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f240180",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0927d",
   "metadata": {},
   "source": [
    "# You can see above that we get 100.00% accuracy for our test dataset. This is considered to be a pretty good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c30200",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a2e8e",
   "metadata": {},
   "source": [
    "# Plotting the Accuracy and Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6422d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e19232",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4126d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss'][:5] # show loss for first 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b959805",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577d00f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1363b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "    \n",
    "    first_image = images_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "    \n",
    "    print(\"first image to predict\")\n",
    "    plt.imshow(first_image)\n",
    "    print(\"actual label:\",class_names[first_label])\n",
    "    \n",
    "    batch_prediction = model.predict(images_batch)\n",
    "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8765dfc",
   "metadata": {},
   "source": [
    "# Write a function for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e62ad",
   "metadata": {},
   "source": [
    "# Now run inference on few sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8209e80",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        actual_class = class_names[labels[i]] \n",
    "        \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4064b6",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "We append the model to the list of models as a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_version=max([int(i) for i in os.listdir(\"../models\") + [0]])+1\n",
    "\n",
    "model.save(f\"./saved_models/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0e31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bcff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e673f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ee70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6207a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475c82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853653e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca81c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e22642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83691451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
